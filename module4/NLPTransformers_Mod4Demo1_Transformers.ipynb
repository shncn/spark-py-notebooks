{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "ekM1n0-JdYPN",
        "OFbNft2shTPp",
        "vwmp1jC0dxVJ",
        "yvlTAaqqhTBo",
        "L5Toc-Uukh15",
        "J76TA2ZMlrke",
        "PuNqXT82oaRo",
        "c8_jJCg8pE6b",
        "1Pm7uGPMpqwu",
        "-MzC1z-vuuxF",
        "UR4MuroWw4p-",
        "pog5S-gQz9nF"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shncn/spark-py-notebooks/blob/master/module4/NLPTransformers_Mod4Demo1_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers\n",
        "\n",
        "© Data Trainers LLC. GPL v 3.0.\n",
        "\n",
        "Author: Axel Sirota\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Inspired highly on the tutorial [NMT with Transformers](https://www.tensorflow.org/text/tutorials/transformer) which takes the code from the original Transformer model paper originally proposed in [\"Attention is all you need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017)."
      ],
      "metadata": {
        "id": "gzf37X_XizQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prep"
      ],
      "metadata": {
        "id": "ovyaAhLpa55P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U nltk keras-nlp keras-preprocessing gensim\n",
        "!pip install tensorflow-text==2.19.0\n"
      ],
      "metadata": {
        "id": "C9BTEOu0PerV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "ad7096cf-127a-40c0-bc03-221230fe5061"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: keras-nlp in /usr/local/lib/python3.12/dist-packages (0.21.1)\n",
            "Collecting keras-nlp\n",
            "  Downloading keras_nlp-0.25.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting keras-preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Collecting keras-hub==0.25.0 (from keras-nlp)\n",
            "  Downloading keras_hub-0.25.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: keras>=3.8 in /usr/local/lib/python3.12/dist-packages (from keras-hub==0.25.0->keras-nlp) (3.10.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras-hub==0.25.0->keras-nlp) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras-hub==0.25.0->keras-nlp) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras-hub==0.25.0->keras-nlp) (25.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras-hub==0.25.0->keras-nlp) (13.9.4)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (from keras-hub==0.25.0->keras-nlp) (0.3.13)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.12/dist-packages (from keras-hub==0.25.0->keras-nlp) (2.19.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from keras-preprocessing) (1.17.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.8->keras-hub==0.25.0->keras-nlp) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras>=3.8->keras-hub==0.25.0->keras-nlp) (3.15.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.8->keras-hub==0.25.0->keras-nlp) (0.18.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras>=3.8->keras-hub==0.25.0->keras-nlp) (0.5.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub->keras-hub==0.25.0->keras-nlp) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub->keras-hub==0.25.0->keras-nlp) (2.32.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras-hub==0.25.0->keras-nlp) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras-hub==0.25.0->keras-nlp) (2.19.2)\n",
            "Requirement already satisfied: tensorflow<2.20,>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-text->keras-hub==0.25.0->keras-nlp) (2.19.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras-hub==0.25.0->keras-nlp) (0.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.25.0->keras-nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.25.0->keras-nlp) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.25.0->keras-nlp) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.25.0->keras-nlp) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.25.0->keras-nlp) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.25.0->keras-nlp) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.25.0->keras-nlp) (5.29.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.25.0->keras-nlp) (75.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.25.0->keras-nlp) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.25.0->keras-nlp) (4.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.25.0->keras-nlp) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.25.0->keras-nlp) (2.19.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->keras-hub==0.25.0->keras-nlp) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->keras-hub==0.25.0->keras-nlp) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->keras-hub==0.25.0->keras-nlp) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->keras-hub==0.25.0->keras-nlp) (2025.11.12)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.25.0->keras-nlp) (0.45.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.25.0->keras-nlp) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.25.0->keras-nlp) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.25.0->keras-nlp) (3.1.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.25.0->keras-nlp) (3.0.3)\n",
            "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras_nlp-0.25.0-py3-none-any.whl (1.8 kB)\n",
            "Downloading keras_hub-0.25.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nltk, keras-preprocessing, gensim, keras-hub, keras-nlp\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "  Attempting uninstall: keras-hub\n",
            "    Found existing installation: keras-hub 0.21.1\n",
            "    Uninstalling keras-hub-0.21.1:\n",
            "      Successfully uninstalled keras-hub-0.21.1\n",
            "  Attempting uninstall: keras-nlp\n",
            "    Found existing installation: keras-nlp 0.21.1\n",
            "    Uninstalling keras-nlp-0.21.1:\n",
            "      Successfully uninstalled keras-nlp-0.21.1\n",
            "Successfully installed gensim-4.4.0 keras-hub-0.25.0 keras-nlp-0.25.0 keras-preprocessing-1.1.2 nltk-3.9.2\n",
            "Requirement already satisfied: tensorflow-text==2.19.0 in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: tensorflow<2.20,>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-text==2.19.0) (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (3.1.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wSaLeBcpqiT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0fpgYwAtNO2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfbff083-524c-47ae-8295-6a8e7acddfcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import warnings\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import gensim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from textblob import TextBlob, Word\n",
        "\n",
        "# ===== TF / KERAS SAFE IMPORTS (2.19) =====\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        "    Embedding,\n",
        "    Lambda,\n",
        "    ELU,\n",
        "    Conv1D,\n",
        "    MaxPooling1D,\n",
        "    Dropout,\n",
        "    Input,\n",
        "    TextVectorization\n",
        ")\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.initializers import Constant\n",
        "\n",
        "# TensorFlow Text (version matches TF exactly)\n",
        "import tensorflow_text as tf_text\n",
        "# ========================================\n",
        "\n",
        "TRACE = False\n",
        "\n",
        "def set_seeds_and_trace():\n",
        "    os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "    random.seed(42)\n",
        "\n",
        "    if TRACE:\n",
        "        tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "def configure_runtime():\n",
        "    \"\"\"\n",
        "    TF 2.x replacement for TF1 Session / ConfigProto.\n",
        "    \"\"\"\n",
        "    # Enable dynamic GPU memory growth\n",
        "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "    # Threading configuration\n",
        "    cores = multiprocessing.cpu_count()\n",
        "    tf.config.threading.set_intra_op_parallelism_threads(cores)\n",
        "    tf.config.threading.set_inter_op_parallelism_threads(cores)\n",
        "\n",
        "# ===== INIT =====\n",
        "set_seeds_and_trace()\n",
        "configure_runtime()\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download(\"punkt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Transformer Layers\n",
        "\n",
        "In this demo we will create, from scratch, with the same tools the original Authors had, the Transformer architecture. Why? To understand how it works, why it works, and exactly what is novel!\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The original Transformer diagram</th>\n",
        "  <th colspan=1>A representation of a 4-layer Transformer</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\"/>\n",
        "  </td>\n",
        "  <td>\n",
        "   <img width=307 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-4layer-compact.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "Each of the components in these two diagrams will be explained as you progress through the demo.\n"
      ],
      "metadata": {
        "id": "ekM1n0-JdYPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did we have before?"
      ],
      "metadata": {
        "id": "OFbNft2shTPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before, we used Cross Attention or self attention, remember? And for sequence data we basically used it like this:\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>Seq2Seq with attention</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.dropbox.com/s/r6u7ll5nlt96t9f/seq2seq.png?raw=1\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>\n",
        "\n"
      ],
      "metadata": {
        "id": "Q2FoDkeGhWJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where we input attention with the hidden state to create another updated hidden state we could input into the next cell. And this worked well on medium sized sentences, but was hard to train and unstable. Now that we know this, the Transformer basicaly tried to get rid of the RNN by using **only** attention"
      ],
      "metadata": {
        "id": "b3huYRrUjCAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The embedding and positional encoding layer\n",
        "\n",
        "The inputs to both the encoder and decoder use the same embedding and positional encoding logic.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The embedding and positional encoding layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/PositionalEmbedding.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "vwmp1jC0dxVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## This comes straight from the paper\n",
        "\n",
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "UkI09F6zdXnb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x"
      ],
      "metadata": {
        "id": "gLC8RPIkd--M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = PositionalEmbedding(5000, 100)"
      ],
      "metadata": {
        "id": "GAnc5AKSeOvn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = tf.constant(np.random.randint(1,5000, size=(3,26)))\n",
        "response = pos(input)\n",
        "response.shape"
      ],
      "metadata": {
        "id": "zlIZmHLMemkR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a1608ff-8f72-4be6-be69-9de9f9bac5d3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 26, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response._keras_mask"
      ],
      "metadata": {
        "id": "XnIEkVWqg59O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbc46f39-f820-4122-aa67-1f3d6fe64131"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 26), dtype=bool, numpy=\n",
              "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True]])>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add and normalize\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=2>Add and normalize</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Add+Norm.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "yvlTAaqqhTBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Use `Add` layer instead of + to propagate masks\n",
        "\n",
        "We will create a BaseAttention layer that inherits the Add+Norm and then each subclass of attention will implement the correct one"
      ],
      "metadata": {
        "id": "P-ZTlyygheuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ],
      "metadata": {
        "id": "OMMyVtc6hLaM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self Attention layer\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The global self attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/SelfAttention.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "L5Toc-Uukh15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    # We need to compare everything with everything, therefore Q, K and V must be the input\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])  # This one comes from the base class\n",
        "    x = self.layernorm(x)  # This one comes from the base class\n",
        "    return x"
      ],
      "metadata": {
        "id": "RP4L1Mn-iFCl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it!"
      ],
      "metadata": {
        "id": "316Iwonjm6JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "vocab_size = 5000\n",
        "input = tf.constant(np.random.randint(1,vocab_size, size=(3,26)))\n",
        "\n",
        "# First we apply the PositionalEmbedding to embed into what the attention layer expects\n",
        "pos = PositionalEmbedding(vocab_size, embedding_dim)\n",
        "\n",
        "# Then we do the self attention, the n_heads is arbitrary\n",
        "gsa = GlobalSelfAttention(num_heads=3, key_dim=embedding_dim)\n",
        "\n",
        "\n",
        "response = gsa(pos(input))\n",
        "response.shape"
      ],
      "metadata": {
        "id": "goX_DO0Sk8UB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e608d136-6846-433e-87fa-e5912f80053b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 26, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the shape is the same, since MHA concats all 3 heads and the we add everything"
      ],
      "metadata": {
        "id": "n5lGk8ZMle7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The cross attention layer\n",
        "\n",
        "This layer connects the encoder and decoder. This layer is the most straight-forward use of attention in the model, it performs the same task as the attention block in the previous demo (and we will copy it).\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The cross attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "J76TA2ZMlrke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,  # This is the key part!!\n",
        "        value=context,  # This is the key part!!\n",
        "        return_attention_scores=True)\n",
        "\n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "OxZMLVBRlMPz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim_es = 100\n",
        "vocab_size_es = 5000\n",
        "\n",
        "embedding_dim_en = 512\n",
        "vocab_size_en = 6000\n",
        "\n",
        "# We are supposing the model will translate Spanish to English, so context for CrossAttention will be the spanish input.\n",
        "\n",
        "input_es = tf.constant(np.random.randint(1,vocab_size_es, size=(3,26)))\n",
        "input_en = tf.constant(np.random.randint(1,vocab_size_es, size=(3,24)))\n",
        "\n",
        "\n",
        "pos_es = PositionalEmbedding(vocab_size_es, embedding_dim_es)\n",
        "pos_en = PositionalEmbedding(vocab_size_en, embedding_dim_en)\n",
        "\n",
        "\n",
        "gsa = GlobalSelfAttention(num_heads=3, key_dim=embedding_dim_es)\n",
        "cross = CrossAttention(num_heads=3, key_dim=embedding_dim_en)\n",
        "\n",
        "\n",
        "context = gsa(pos_es(input_es)) # Forget about the feed forwards\n",
        "\n",
        "response = cross(pos_en(input_en), context=context) # Forget about masked attention for now, assume it is the identity\n",
        "\n",
        "response.shape"
      ],
      "metadata": {
        "id": "R9v9MPbvmVJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbf7be7e-f51a-42b8-8361-22f3174d825c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 24, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the shape is (batch_size, words in sentence in output, embedding_dim) , regardless the input sentence had more words or other embedding dim. We are doing a good move forward!"
      ],
      "metadata": {
        "id": "e5bXoAwvoD8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The causal self attention layer (Masked Multi Headed Attention)\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The causal self attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>\n",
        "\n"
      ],
      "metadata": {
        "id": "PuNqXT82oaRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The only big difference in the masked multi headedd attention is that we cannot attend to words in the future, so we will use a mask such that the `Nth` word can only see the first `N-1` words and not all the sentence."
      ],
      "metadata": {
        "id": "_-jDIHkJowAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)  # This is the key!\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "z6N9U3qvn9Ui"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The causal self attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=330 src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention-new-full.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "PgCBG1b-osuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice in the diagram above how the query can onlly attend the values for the past"
      ],
      "metadata": {
        "id": "AdE_KY2TpEPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim_en = 512\n",
        "vocab_size_en = 6000\n",
        "\n",
        "# We are supposing the model will translate Spanish to English, so context for CrossAttention will be the spanish input.\n",
        "\n",
        "input_en = tf.constant(np.random.randint(1,vocab_size_es, size=(3,24)))\n",
        "\n",
        "\n",
        "pos_en = PositionalEmbedding(vocab_size_en, embedding_dim_en)\n",
        "\n",
        "csa = CausalSelfAttention(num_heads =3, key_dim=embedding_dim_en)\n",
        "\n",
        "response = csa(pos_es(input_en))\n",
        "\n",
        "response.shape"
      ],
      "metadata": {
        "id": "RaHIa0k4oqD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee190d96-8f65-4aef-f1ba-b815cabd1781"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 24, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The feed forward network\n",
        "\n",
        "The transformer also includes this point-wise feed-forward network in both the encoder and decoder:\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The feed forward network</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/FeedForward.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "c8_jJCg8pE6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "Wu5xFoRjpBty"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The encoder layer\n",
        "\n",
        "The encoder contains a stack of `N` encoder layers. Where each `EncoderLayer` contains a `GlobalSelfAttention` and `FeedForward` layer:\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The encoder layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/EncoderLayer.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "1Pm7uGPMpqwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "3SJl4aJ_pSR0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "vocab_size = 5000\n",
        "input = tf.constant(np.random.randint(1,vocab_size, size=(3,26)))\n",
        "pos = PositionalEmbedding(vocab_size, embedding_dim)\n",
        "sample_encoder_layer = EncoderLayer(d_model=embedding_dim, num_heads=3, dff=1012)\n",
        "response = sample_encoder_layer(pos(input))\n",
        "response.shape"
      ],
      "metadata": {
        "id": "4NUNWld5uOBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd4cf850-5e35-4c72-d757-9dfaa788da35"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 26, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The encoder\n",
        "\n",
        "Notice we need to be able to repeat the past EncoderLayer Nx times, so we need another Layer that is able to do exactly that\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The encoder</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Encoder.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "-MzC1z-vuuxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`."
      ],
      "metadata": {
        "id": "0TlZP_vVulm0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "vocab_size = 5000\n",
        "input = tf.constant(np.random.randint(1,vocab_size, size=(3,26)))\n",
        "sample_encoder = Encoder(num_layers=4,\n",
        "                         d_model=embedding_dim,\n",
        "                         num_heads=3,\n",
        "                         dff=512,\n",
        "                         vocab_size=vocab_size)\n",
        "response = sample_encoder(input)\n",
        "response.shape"
      ],
      "metadata": {
        "id": "Yf9wGOSMwaEt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a455bb3-1bfc-4499-dc80-6a1f7a20ba23"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 26, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got our Encoder!! Yahoo!!"
      ],
      "metadata": {
        "id": "uUU9CG_MwtJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The decoder layer\n",
        "\n",
        "Same as before we need a Decoder layer that uses the Attention layers and then another layer to permit having Nx layers of decoding\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The decoder layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/DecoderLayer.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "UR4MuroWw4p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x"
      ],
      "metadata": {
        "id": "_v5voBr2wywC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim_es = 100\n",
        "vocab_size_es = 5000\n",
        "\n",
        "embedding_dim_en = 512\n",
        "vocab_size_en = 6000\n",
        "\n",
        "# We are supposing the model will translate Spanish to English, so context for CrossAttention will be the spanish input.\n",
        "\n",
        "input_es = tf.constant(np.random.randint(1,vocab_size_es, size=(3,26)))\n",
        "input_en = tf.constant(np.random.randint(1,vocab_size_es, size=(3,24)))\n",
        "\n",
        "pos_en = PositionalEmbedding(vocab_size_en, embedding_dim_en)\n",
        "\n",
        "\n",
        "encoder =  Encoder(num_layers=2, d_model=embedding_dim_es, num_heads=3, dff=512, vocab_size=vocab_size_es)\n",
        "\n",
        "context = encoder(input_es)\n",
        "\n",
        "decoder_layer = DecoderLayer(d_model=embedding_dim_en, num_heads=3, dff=218, dropout_rate=0.2)\n",
        "\n",
        "response = decoder_layer(pos_en(input_en), context=context)\n",
        "\n",
        "response.shape"
      ],
      "metadata": {
        "id": "aP3yjQeuy2n4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeb71602-f7df-455e-e494-77dd6675fe71"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 24, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Decoder\n",
        "\n",
        "Similar to the `Encoder`, the `Decoder` consists of a `PositionalEmbedding`, and a stack of `DecoderLayer`s:\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The embedding and positional encoding layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Decoder.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "pog5S-gQz9nF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x"
      ],
      "metadata": {
        "id": "dEZ2-NRbz1jz"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim_es = 100\n",
        "vocab_size_es = 5000\n",
        "\n",
        "embedding_dim_en = 512\n",
        "vocab_size_en = 6000\n",
        "\n",
        "# We are supposing the model will translate Spanish to English, so context for CrossAttention will be the spanish input.\n",
        "\n",
        "input_es = tf.constant(np.random.randint(1,vocab_size_es, size=(3,26)))\n",
        "input_en = tf.constant(np.random.randint(1,vocab_size_es, size=(3,24)))\n",
        "\n",
        "encoder =  Encoder(num_layers=2, d_model=embedding_dim_es, num_heads=3, dff=512, vocab_size=vocab_size_es)\n",
        "\n",
        "context = encoder(input_es)\n",
        "\n",
        "decoder = Decoder(num_layers=3, d_model=embedding_dim_en, num_heads=5, dff=124, vocab_size=vocab_size_en)\n",
        "\n",
        "response = decoder(input_en, context=context)\n",
        "\n",
        "response.shape"
      ],
      "metadata": {
        "id": "Mp4zkTw22-23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "506ca04c-5e0a-4af4-902a-f7343fc781c0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 24, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Transformer Model\n",
        "\n",
        "You now have `Encoder` and `Decoder`. To complete the `Transformer` model, you need to put them together and add a final linear (`Dense`) layer which converts the resulting vector at each location into output token probabilities.\n",
        "\n",
        "The output of the decoder is the input to this final linear layer.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The transformer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "3r2JLfz04Hsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=input_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "    context, x  = inputs\n",
        "\n",
        "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
        "\n",
        "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
        "\n",
        "    # Final linear layer output.\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits"
      ],
      "metadata": {
        "id": "VrXSLwrF36u_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "vocab_size_es = 5000\n",
        "vocab_size_en = 6000\n",
        "\n",
        "num_layers = 4\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# We are supposing the model will translate Spanish to English, so context for CrossAttention will be the spanish input.\n",
        "\n",
        "input_es = tf.constant(np.random.randint(1,vocab_size_es, size=(3,26)))\n",
        "input_en = tf.constant(np.random.randint(1,vocab_size_es, size=(3,24)))\n",
        "\n",
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=embedding_dim,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=vocab_size_es,\n",
        "    target_vocab_size=vocab_size_en,\n",
        "    dropout_rate=dropout_rate)\n",
        "\n",
        "response = transformer((input_es, input_en))\n",
        "response.shape"
      ],
      "metadata": {
        "id": "7CIxbfVx9IBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a46efd0-871d-41fe-9fca-ff20e88955d1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 24, 6000])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.summary()\n"
      ],
      "metadata": {
        "id": "Gy5B7d_E-NDE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "4e0c3cf0-933f-47e3-bd04-0169c00b9a37"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"transformer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ encoder_3 (\u001b[38;5;33mEncoder\u001b[0m)             │ ?                      │     \u001b[38;5;34m2,203,648\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_1 (\u001b[38;5;33mDecoder\u001b[0m)             │ ?                      │     \u001b[38;5;34m3,594,448\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_42 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m6000\u001b[0m)          │       \u001b[38;5;34m606,000\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ encoder_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,203,648</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,594,448</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6000</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">606,000</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,404,096\u001b[0m (24.43 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,404,096</span> (24.43 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,404,096\u001b[0m (24.43 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,404,096</span> (24.43 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gkzGKLjN-T9k"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bbb2cad"
      },
      "source": [
        "# Task\n",
        "Select and load a suitable public machine translation dataset using `tf.keras.utils.get_file` to be used for training the Transformer model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9360d28"
      },
      "source": [
        "## Select and Load Dataset\n",
        "\n",
        "### Subtask:\n",
        "Identify and load a suitable public dataset for machine translation (e.g., a multi-language corpus) that can be used to train the Transformer model. We will download the dataset using `tf.keras.utils.get_file` and then load it into the notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4030885a"
      },
      "source": [
        "#### 1. Define the URL for the English-Spanish dataset\n",
        "\n",
        "We will use the English-Spanish dataset from [ManyThings.org](http://www.manythings.org/anki/) which provides sentence pairs for various languages. Specifically, we'll target the `spa-eng.zip` file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dec81d58"
      },
      "source": [
        "# Task\n",
        "Download and extract the \"spa-eng.zip\" dataset from \"http://www.manythings.org/anki/spa-eng.zip\" using `tf.keras.utils.get_file`, and then load the \"spa.txt\" file into memory as English-Spanish sentence pairs, confirming its readiness for Transformer model preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "272210b7"
      },
      "source": [
        "## Download and Extract Dataset\n",
        "\n",
        "### Subtask:\n",
        "Execute the code cell that uses `tf.keras.utils.get_file` to download and extract the `spa-eng.zip` file, and obtain the path to the `spa.txt` file containing the English-Spanish sentence pairs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef63b061"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires downloading and extracting a zip file containing the dataset and then identifying the path to the main data file. `tf.keras.utils.get_file` is the appropriate function for this, and `os.path.join` will help construct the full file path.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34187b75"
      },
      "source": [
        "# Task\n",
        "Download and extract the \"spa-eng.zip\" dataset from \"http://www.manythings.org/anki/spa-eng.zip\" using the `requests` library with a `User-Agent` header and Python's `zipfile` module. Then, load the \"spa.txt\" file into memory as English-Spanish sentence pairs, confirming its readiness for Transformer model preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c1f83c9"
      },
      "source": [
        "## Download and Extract Dataset (Revised)\n",
        "\n",
        "### Subtask:\n",
        "Download the 'spa-eng.zip' dataset using the `requests` library with a `User-Agent` header, then extract the 'spa.txt' file using Python's `zipfile` module and store its path.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a7f056d"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires downloading a zip file using `requests` with a `User-Agent` header, saving it locally, then extracting a specific file (`spa.txt`) from it using `zipfile`. This code block will perform all these steps sequentially.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c634480",
        "outputId": "b9ddf43a-2684-406a-ab84-888be54f9e0d"
      },
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the URL for the dataset\n",
        "url = 'http://www.manythings.org/anki/spa-eng.zip'\n",
        "\n",
        "# Define a User-Agent header\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "\n",
        "# Define the local path for the zip file\n",
        "zip_file_name = 'spa-eng.zip'\n",
        "\n",
        "# Make an HTTP GET request to the URL with the User-Agent header\n",
        "print(f\"Downloading {url}...\")\n",
        "response = requests.get(url, headers=headers, stream=True)\n",
        "response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "# Save the content of the response to a local zip file\n",
        "with open(zip_file_name, 'wb') as f:\n",
        "    for chunk in response.iter_content(chunk_size=8192):\n",
        "        f.write(chunk)\n",
        "print(f\"Downloaded {zip_file_name}\")\n",
        "\n",
        "# Open the downloaded zip file and extract 'spa.txt'\n",
        "path_to_file = None\n",
        "with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
        "    # List all files in the zip archive to find 'spa.txt'\n",
        "    file_list = zip_ref.namelist()\n",
        "    if 'spa.txt' in file_list:\n",
        "        zip_ref.extract('spa.txt', path='.') # Extract to current directory\n",
        "        path_to_file = os.path.join('.', 'spa.txt')\n",
        "        print(f\"Extracted spa.txt to {path_to_file}\")\n",
        "    else:\n",
        "        print(\"'spa.txt' not found in the zip archive.\")\n",
        "\n",
        "# Confirm that path_to_file is set\n",
        "if path_to_file:\n",
        "    print(f\"Path to the extracted dataset file: {path_to_file}\")\n",
        "else:\n",
        "    print(\"Extraction failed or 'spa.txt' was not found.\")\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://www.manythings.org/anki/spa-eng.zip...\n",
            "Downloaded spa-eng.zip\n",
            "Extracted spa.txt to ./spa.txt\n",
            "Path to the extracted dataset file: ./spa.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07ac6787"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully downloaded and extracted 'spa.txt' and stored its path. The next logical step is to load the content of this file into memory, as required by the subtask, to prepare for processing the English-Spanish sentence pairs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32602f4a",
        "outputId": "15bbf63e-2bd3-4f0a-c6eb-cc0af7a5e2c4"
      },
      "source": [
        "import io\n",
        "\n",
        "# Read the content of the extracted 'spa.txt' file\n",
        "with io.open(path_to_file, mode='r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "# Display the first few lines to verify\n",
        "print(f\"Total lines loaded: {len(lines)}\")\n",
        "print(\"\\nFirst 5 lines:\")\n",
        "for i, line in enumerate(lines[:5]):\n",
        "    print(f\"{i+1}: {line}\")\n",
        "\n",
        "# Store lines in a variable for further use\n",
        "dataset_lines = lines"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total lines loaded: 142929\n",
            "\n",
            "First 5 lines:\n",
            "1: Go.\tVe.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #4986655 (cueyayotl)\n",
            "2: Go.\tVete.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #4986656 (cueyayotl)\n",
            "3: Go.\tVaya.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #4986657 (cueyayotl)\n",
            "4: Go.\tVáyase.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #6586271 (arh)\n",
            "5: Hi.\tHola.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #431975 (Leono)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "679ebd5c"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Confirm that the English-Spanish dataset has been successfully loaded into memory and is ready for the next phase of preprocessing for the Transformer model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31735508"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "Yes, the English-Spanish dataset has been successfully loaded into memory and is ready for the next phase of preprocessing for the Transformer model.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `spa-eng.zip` file was successfully downloaded from the specified URL using `requests` with a `User-Agent` header.\n",
        "*   The `spa.txt` file was extracted from the downloaded zip archive to the current directory, and its path (`./spa.txt`) was successfully identified and stored.\n",
        "*   The extracted `spa.txt` file was successfully read into memory, containing `142929` lines.\n",
        "*   The first few lines confirmed the correct loading of English-Spanish sentence pairs, such as \"Go.\\tVe.\", verifying the data structure.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The dataset is now in memory (`dataset_lines` variable) and correctly formatted as English-Spanish sentence pairs, making it suitable for the next phase of preprocessing for a Transformer model, which typically involves tokenization, numericalization, and batching.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e726a3d4"
      },
      "source": [
        "# Task\n",
        "Parse and clean the `dataset_lines` by extracting English and Spanish sentence pairs, converting text to lowercase, removing special characters, and adding `[START]` and `[END]` tokens to each sentence. Store the cleaned English and Spanish sentences in separate lists."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acfabcaf"
      },
      "source": [
        "## Parse and Clean Dataset\n",
        "\n",
        "### Subtask:\n",
        "Iterate through the `dataset_lines` to extract English and Spanish sentence pairs. Clean the text by converting to lowercase, removing unwanted characters, and adding start and end tokens (e.g., '[START]' and '[END]') to both English and Spanish sentences. Store these cleaned pairs in separate lists.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "814d04fa"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to implement the detailed instructions for parsing, cleaning, and tokenizing the English and Spanish sentences from the `dataset_lines` list. This involves iterating, splitting, applying regex for cleaning, adding start/end tokens, and storing them in new lists. Finally, printing a sample to verify.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d53a55d9",
        "outputId": "e46d8e89-a059-44c8-a530-cc494aef5ba4"
      },
      "source": [
        "import re\n",
        "\n",
        "# 1. Initialize two empty lists\n",
        "english_sentences = []\n",
        "spanish_sentences = []\n",
        "\n",
        "# Define a cleaning function\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    # Remove characters that are not letters, numbers, basic punctuation, or spaces\n",
        "    # Keep periods, commas, question marks, exclamation marks, and apostrophes\n",
        "    text = re.sub(r\"[^a-zA-Z0-9.,?!'\\s]\", '', text)\n",
        "    text = text.strip() # Remove leading/trailing whitespace\n",
        "    return text\n",
        "\n",
        "# 2. Iterate through each line in the dataset_lines list\n",
        "for line in dataset_lines:\n",
        "    # 3. For each line, split it by the tab character\n",
        "    parts = line.split('\\t')\n",
        "\n",
        "    # Ensure that only lines with at least two parts (English and Spanish) are processed\n",
        "    if len(parts) >= 2:\n",
        "        # 4. Extract the English sentence (first part) and the Spanish sentence (second part)\n",
        "        eng_sentence_raw = parts[0]\n",
        "        spa_sentence_raw = parts[1]\n",
        "\n",
        "        # 5. Convert both the extracted English and Spanish sentences to lowercase and clean\n",
        "        cleaned_eng = clean_text(eng_sentence_raw)\n",
        "        cleaned_spa = clean_text(spa_sentence_raw)\n",
        "\n",
        "        # 7. Add the token \"[START]\" at the beginning and \"[END]\" at the end\n",
        "        final_eng = \"[START] \" + cleaned_eng + \" [END]\"\n",
        "        final_spa = \"[START] \" + cleaned_spa + \" [END]\"\n",
        "\n",
        "        # 8. Append the processed sentences to their respective lists\n",
        "        english_sentences.append(final_eng)\n",
        "        spanish_sentences.append(final_spa)\n",
        "\n",
        "# 9. After the loop, print the first 5 entries of both lists to verify\n",
        "print(f\"Total English sentences processed: {len(english_sentences)}\")\n",
        "print(f\"Total Spanish sentences processed: {len(spanish_sentences)}\")\n",
        "\n",
        "print(\"\\nFirst 5 English sentences:\")\n",
        "for i, sentence in enumerate(english_sentences[:5]):\n",
        "    print(f\"{i+1}: {sentence}\")\n",
        "\n",
        "print(\"\\nFirst 5 Spanish sentences:\")\n",
        "for i, sentence in enumerate(spanish_sentences[:5]):\n",
        "    print(f\"{i+1}: {sentence}\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total English sentences processed: 142928\n",
            "Total Spanish sentences processed: 142928\n",
            "\n",
            "First 5 English sentences:\n",
            "1: [START] go. [END]\n",
            "2: [START] go. [END]\n",
            "3: [START] go. [END]\n",
            "4: [START] go. [END]\n",
            "5: [START] hi. [END]\n",
            "\n",
            "First 5 Spanish sentences:\n",
            "1: [START] ve. [END]\n",
            "2: [START] vete. [END]\n",
            "3: [START] vaya. [END]\n",
            "4: [START] vyase. [END]\n",
            "5: [START] hola. [END]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6219651"
      },
      "source": [
        "## Create TextVectorization Layers for English and Spanish\n",
        "\n",
        "### Subtask:\n",
        "Initialize two `tf.keras.layers.TextVectorization` layers, one for the English sentences and one for the Spanish sentences. Configure them to output integer sequences. Adapt each layer to its respective cleaned text list to build the vocabulary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e23a0fb2"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires initializing two `TextVectorization` layers, adapting them to the English and Spanish sentences, and then printing their vocabulary sizes. This code block will perform all these steps as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20275c0f",
        "outputId": "bb64dee8-a899-423d-a5bd-66052821692d"
      },
      "source": [
        "english_vectorizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=15000, # Set a suitable vocabulary size for English\n",
        "    standardize=\"strip_punctuation\",\n",
        "    split=\"whitespace\",\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=None # Keep variable length for now\n",
        ")\n",
        "\n",
        "english_vectorizer.adapt(english_sentences)\n",
        "\n",
        "spanish_vectorizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=20000, # Set a suitable vocabulary size for Spanish\n",
        "    standardize=\"strip_punctuation\",\n",
        "    split=\"whitespace\",\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=None # Keep variable length for now\n",
        ")\n",
        "\n",
        "spanish_vectorizer.adapt(spanish_sentences)\n",
        "\n",
        "print(f\"English Vocabulary Size: {len(english_vectorizer.get_vocabulary())}\")\n",
        "print(f\"Spanish Vocabulary Size: {len(spanish_vectorizer.get_vocabulary())}\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Vocabulary Size: 14574\n",
            "Spanish Vocabulary Size: 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "787087e6"
      },
      "source": [
        "## Verify Tokenizer Functionality\n",
        "\n",
        "### Subtask:\n",
        "Test the created `TextVectorization` layers by taking a few sample sentences, tokenizing them, and then converting the token IDs back to text to ensure the tokenization and detokenization process works as expected.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0324c36f"
      },
      "source": [
        "**Reasoning**:\n",
        "To verify the functionality of the `TextVectorization` layers, I need to tokenize sample sentences using the vectorizers and then detokenize them back to text to ensure the process is working correctly. This will involve using the `english_vectorizer` and `spanish_vectorizer` with `english_sentences` and `spanish_sentences`, and then using their respective vocabularies for detokenization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f80f2e25",
        "outputId": "aeb2ddea-0c33-4d8d-8ee3-26981adb81de"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Select a few sample English sentences\n",
        "sample_english_sentences = english_sentences[:3]\n",
        "\n",
        "# 2. Select a few sample Spanish sentences\n",
        "sample_spanish_sentences = spanish_sentences[:3]\n",
        "\n",
        "# Get vocabularies for detokenization\n",
        "english_vocab = english_vectorizer.get_vocabulary()\n",
        "spanish_vocab = spanish_vectorizer.get_vocabulary()\n",
        "\n",
        "# Helper function to detokenize\n",
        "def detokenize(token_ids, vocabulary):\n",
        "    # Filter out padding (0) and OOV (1) tokens for clean display, and start/end tokens\n",
        "    words = [vocabulary[token_id] for token_id in token_ids if token_id > 1]\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "print(\"\\n--- English Tokenization Verification ---\")\n",
        "# 3. Tokenize and 5. Detokenize English sentences\n",
        "for i, original_sentence in enumerate(sample_english_sentences):\n",
        "    tokenized_sentence_eng = english_vectorizer([original_sentence])\n",
        "    detokenized_sentence_eng = detokenize(tokenized_sentence_eng[0].numpy(), english_vocab)\n",
        "\n",
        "    print(f\"\\nOriginal English {i+1}: {original_sentence}\")\n",
        "    print(f\"Tokenized English {i+1} (IDs): {tokenized_sentence_eng.numpy()}\")\n",
        "    print(f\"Detokenized English {i+1}: {detokenized_sentence_eng}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Spanish Tokenization Verification ---\")\n",
        "# 4. Tokenize and 6. Detokenize Spanish sentences\n",
        "for i, original_sentence in enumerate(sample_spanish_sentences):\n",
        "    tokenized_sentence_spa = spanish_vectorizer([original_sentence])\n",
        "    detokenized_sentence_spa = detokenize(tokenized_sentence_spa[0].numpy(), spanish_vocab)\n",
        "\n",
        "    print(f\"\\nOriginal Spanish {i+1}: {original_sentence}\")\n",
        "    print(f\"Tokenized Spanish {i+1} (IDs): {tokenized_sentence_spa.numpy()}\")\n",
        "    print(f\"Detokenized Spanish {i+1}: {detokenized_sentence_spa}\")\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- English Tokenization Verification ---\n",
            "\n",
            "Original English 1: [START] go. [END]\n",
            "Tokenized English 1 (IDs): [[ 2 42  3]]\n",
            "Detokenized English 1: START go END\n",
            "\n",
            "Original English 2: [START] go. [END]\n",
            "Tokenized English 2 (IDs): [[ 2 42  3]]\n",
            "Detokenized English 2: START go END\n",
            "\n",
            "Original English 3: [START] go. [END]\n",
            "Tokenized English 3 (IDs): [[ 2 42  3]]\n",
            "Detokenized English 3: START go END\n",
            "\n",
            "--- Spanish Tokenization Verification ---\n",
            "\n",
            "Original Spanish 1: [START] ve. [END]\n",
            "Tokenized Spanish 1 (IDs): [[  2 388   3]]\n",
            "Detokenized Spanish 1: START ve END\n",
            "\n",
            "Original Spanish 2: [START] vete. [END]\n",
            "Tokenized Spanish 2 (IDs): [[   2 1389    3]]\n",
            "Detokenized Spanish 2: START vete END\n",
            "\n",
            "Original Spanish 3: [START] vaya. [END]\n",
            "Tokenized Spanish 3 (IDs): [[  2 492   3]]\n",
            "Detokenized Spanish 3: START vaya END\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e5c8b62"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Confirm that the dataset has been successfully parsed, cleaned, and that the `TextVectorization` layers for English and Spanish have been created and adapted, ready for use in building the input pipelines.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99a29733"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The dataset was successfully parsed and cleaned, resulting in $142,928$ pairs of English and Spanish sentences. Each sentence was converted to lowercase, special characters were removed (while preserving basic punctuation), and `[START]` and `[END]` tokens were appended.\n",
        "*   Two `tf.keras.layers.TextVectorization` layers were created and adapted: `english_vectorizer` with a vocabulary size of $14,574$ and `spanish_vectorizer` with a vocabulary size of $20,000$.\n",
        "*   Verification tests confirmed that both `TextVectorization` layers correctly tokenize sample sentences, including the `[START]` and `[END]` tokens, and that the detokenization process accurately reconstructs the text from token IDs.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The prepared `english_sentences`, `spanish_sentences`, and the adapted `TextVectorization` layers (`english_vectorizer`, `spanish_vectorizer`) are now ready to be integrated into the input pipeline for a neural machine translation model.\n",
        "*   The next logical step is to create a `tf.data.Dataset` using these cleaned and tokenized sentences, applying the vectorization layers to numericalize the text, and then batching and preprocessing the data for model training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17b30a2c"
      },
      "source": [
        "# Task\n",
        "**Task:** Determine the appropriate maximum sequence lengths for English and Spanish sentences based on the cleaned dataset, update the `TextVectorization` layers with these lengths, and define a function to apply these vectorizers to sentence pairs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69d8d758"
      },
      "source": [
        "## Create Input Pipelines\n",
        "\n",
        "### Subtask:\n",
        "Determine the appropriate maximum sequence lengths for English and Spanish sentences based on the cleaned dataset, update the `TextVectorization` layers with these lengths, and define a function to apply these vectorizers to sentence pairs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6803eeb"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to calculate the length of each English and Spanish sentence to determine appropriate maximum sequence lengths for the `TextVectorization` layers. This is essential for padding and truncating sequences consistently.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b61431e2",
        "outputId": "69cb30f1-aa23-4ea7-b001-f59aa50bda2c"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Calculate the length of each sentence\n",
        "english_lengths = [len(s.split()) for s in english_sentences]\n",
        "spanish_lengths = [len(s.split()) for s in spanish_sentences]\n",
        "\n",
        "# 2. Determine maximum and percentile lengths\n",
        "max_eng_len = np.max(english_lengths)\n",
        "max_spa_len = np.max(spanish_lengths)\n",
        "\n",
        "pct95_eng_len = np.percentile(english_lengths, 95)\n",
        "pct95_spa_len = np.percentile(spanish_lengths, 95)\n",
        "\n",
        "pct99_eng_len = np.percentile(english_lengths, 99)\n",
        "pct99_spa_len = np.percentile(spanish_lengths, 99)\n",
        "\n",
        "print(f\"Max English sentence length: {max_eng_len}\")\n",
        "print(f\"95th percentile English sentence length: {pct95_eng_len}\")\n",
        "print(f\"99th percentile English sentence length: {pct99_eng_len}\")\n",
        "\n",
        "print(f\"\\nMax Spanish sentence length: {max_spa_len}\")\n",
        "print(f\"95th percentile Spanish sentence length: {pct95_spa_len}\")\n",
        "print(f\"99th percentile Spanish sentence length: {pct99_spa_len}\")\n",
        "\n",
        "# Choosing a practical maximum length. Let's use the 99th percentile for now,\n",
        "# or the max if it's very close and not an extreme outlier.\n",
        "MAX_SEQ_LENGTH_ENG = int(pct99_eng_len) # or max_eng_len if desired\n",
        "MAX_SEQ_LENGTH_SPA = int(pct99_spa_len) # or max_spa_len if desired\n",
        "\n",
        "print(f\"\\nChosen English max sequence length: {MAX_SEQ_LENGTH_ENG}\")\n",
        "print(f\"Chosen Spanish max sequence length: {MAX_SEQ_LENGTH_SPA}\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max English sentence length: 72\n",
            "95th percentile English sentence length: 13.0\n",
            "99th percentile English sentence length: 16.0\n",
            "\n",
            "Max Spanish sentence length: 70\n",
            "95th percentile Spanish sentence length: 13.0\n",
            "99th percentile Spanish sentence length: 17.0\n",
            "\n",
            "Chosen English max sequence length: 16\n",
            "Chosen Spanish max sequence length: 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98e5afdc"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to re-initialize the `TextVectorization` layers for English and Spanish with the determined maximum sequence lengths (`MAX_SEQ_LENGTH_ENG` and `MAX_SEQ_LENGTH_SPA`) and then re-adapt them to their respective sentences. This will ensure that all vectorized sequences have a consistent length, with padding or truncation applied as necessary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30573b87",
        "outputId": "6d7e3dbf-57f0-4220-bb47-d18f925c7b39"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Re-initialize english_vectorizer with the chosen MAX_SEQ_LENGTH_ENG\n",
        "english_vectorizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=15000, # Keep consistent with previous definition\n",
        "    standardize=\"strip_punctuation\",\n",
        "    split=\"whitespace\",\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_SEQ_LENGTH_ENG # Set the determined max length\n",
        ")\n",
        "\n",
        "# Re-adapt the vectorizer to build the vocabulary and apply the new sequence length\n",
        "english_vectorizer.adapt(english_sentences)\n",
        "\n",
        "# Re-initialize spanish_vectorizer with the chosen MAX_SEQ_LENGTH_SPA\n",
        "spanish_vectorizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=20000, # Keep consistent with previous definition\n",
        "    standardize=\"strip_punctuation\",\n",
        "    split=\"whitespace\",\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_SEQ_LENGTH_SPA # Set the determined max length\n",
        ")\n",
        "\n",
        "# Re-adapt the vectorizer to build the vocabulary and apply the new sequence length\n",
        "spanish_vectorizer.adapt(spanish_sentences)\n",
        "\n",
        "print(f\"Re-initialized English Vocabulary Size: {len(english_vectorizer.get_vocabulary())}\")\n",
        "print(f\"Re-initialized Spanish Vocabulary Size: {len(spanish_vectorizer.get_vocabulary())}\")\n",
        "# print(f\"English TextVectorization output sequence length: {english_vectorizer.output_sequence_length}\")\n",
        "# print(f\"Spanish TextVectorization output sequence length: {spanish_vectorizer.output_sequence_length}\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Re-initialized English Vocabulary Size: 14574\n",
            "Re-initialized Spanish Vocabulary Size: 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83caa51d"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `TextVectorization` layers have been updated with the correct `output_sequence_length`, the next step is to define the `vectorize_pair` function as instructed and then test it with sample sentences to verify its functionality and the shapes of the output tensors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "528ad1f6",
        "outputId": "95e7ace8-b47b-4a72-dddd-853a22b0fee5"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 5. Define a function to apply these vectorizers to sentence pairs\n",
        "def vectorize_pair(english_text, spanish_text):\n",
        "    english_vec = english_vectorizer(english_text)\n",
        "    spanish_vec = spanish_vectorizer(spanish_text)\n",
        "    return english_vec, spanish_vec\n",
        "\n",
        "# 6. Test the vectorize_pair function with a sample English-Spanish sentence pair\n",
        "sample_eng_input = tf.constant(english_sentences[:1])\n",
        "sample_spa_input = tf.constant(spanish_sentences[:1])\n",
        "\n",
        "vectorized_eng, vectorized_spa = vectorize_pair(sample_eng_input, sample_spa_input)\n",
        "\n",
        "print(f\"\\nSample English input: {english_sentences[0]}\")\n",
        "print(f\"Vectorized English output shape: {vectorized_eng.shape}\")\n",
        "print(f\"Vectorized English output (first 5 tokens): {vectorized_eng[0][:5].numpy()}\")\n",
        "\n",
        "print(f\"\\nSample Spanish input: {spanish_sentences[0]}\")\n",
        "print(f\"Vectorized Spanish output shape: {vectorized_spa.shape}\")\n",
        "print(f\"Vectorized Spanish output (first 5 tokens): {vectorized_spa[0][:5].numpy()}\")\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample English input: [START] go. [END]\n",
            "Vectorized English output shape: (1, 16)\n",
            "Vectorized English output (first 5 tokens): [ 2 42  3  0  0]\n",
            "\n",
            "Sample Spanish input: [START] ve. [END]\n",
            "Vectorized Spanish output shape: (1, 17)\n",
            "Vectorized Spanish output (first 5 tokens): [  2 388   3   0   0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24dba549"
      },
      "source": [
        "## Create Input Pipelines\n",
        "\n",
        "### Subtask:\n",
        "Construct `tf.data` input pipelines for training and validation by tokenizing the cleaned English and Spanish sentence pairs, splitting the dataset, batching the data, and prefetching.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24d3564c"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to create a `tf.data.Dataset` from the `english_sentences` and `spanish_sentences` lists and then apply the `vectorize_pair` function using `.map()` to tokenize and pad the sentences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27c03684",
        "outputId": "4a52c84e-e60f-42fa-c48a-542dfa4e715a"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Create a tf.data.Dataset from the english_sentences and spanish_sentences lists\n",
        "dataset = tf.data.Dataset.from_tensor_slices((english_sentences, spanish_sentences))\n",
        "\n",
        "# 2. Apply the vectorize_pair function to the dataset using .map() to tokenize and pad the sentences\n",
        "dataset = dataset.map(vectorize_pair)\n",
        "\n",
        "print(\"Dataset created and vectorized.\")\n",
        "print(f\"Number of elements in dataset: {tf.data.experimental.cardinality(dataset).numpy()}\")\n",
        "\n",
        "# Inspect one element to ensure correct vectorization and shape\n",
        "for eng, spa in dataset.take(1):\n",
        "    print(f\"\\nSample vectorized English sentence shape: {eng.shape}\")\n",
        "    print(f\"Sample vectorized English sentence (first 5 tokens): {eng[:5].numpy()}\")\n",
        "    print(f\"Sample vectorized Spanish sentence shape: {spa.shape}\")\n",
        "    print(f\"Sample vectorized Spanish sentence (first 5 tokens): {spa[:5].numpy()}\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created and vectorized.\n",
            "Number of elements in dataset: 142928\n",
            "\n",
            "Sample vectorized English sentence shape: (16,)\n",
            "Sample vectorized English sentence (first 5 tokens): [ 2 42  3  0  0]\n",
            "Sample vectorized Spanish sentence shape: (17,)\n",
            "Sample vectorized Spanish sentence (first 5 tokens): [  2 388   3   0   0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30b0de62"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the dataset is created and vectorized, the next logical step is to split it into training and validation sets as per the instructions, setting an appropriate split ratio.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a57bb57a",
        "outputId": "990f713d-d472-4b4d-c68c-ec4fc56bfb00"
      },
      "source": [
        "BUFFER_SIZE = tf.data.experimental.cardinality(dataset).numpy()\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# 3. Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * BUFFER_SIZE)\n",
        "val_size = BUFFER_SIZE - train_size\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE, reshuffle_each_iteration=False)\n",
        "\n",
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size)\n",
        "\n",
        "print(f\"Total dataset size: {BUFFER_SIZE}\")\n",
        "print(f\"Training dataset size: {tf.data.experimental.cardinality(train_dataset).numpy()}\")\n",
        "print(f\"Validation dataset size: {tf.data.experimental.cardinality(val_dataset).numpy()}\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total dataset size: 142928\n",
            "Training dataset size: 114342\n",
            "Validation dataset size: 28586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb6a712b"
      },
      "source": [
        "**Reasoning**:\n",
        "With the dataset split into training and validation sets, the next step is to define the `make_batches` function to optimize performance by applying caching, shuffling, batching, and prefetching as per instruction 4.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2014ec5",
        "outputId": "e35f54e8-cdf3-484d-9119-d29cb34e99b5"
      },
      "source": [
        "BUFFER_SIZE = tf.data.experimental.cardinality(dataset).numpy()\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# 4. Define a make_batches function\n",
        "def make_batches(ds):\n",
        "    return (\n",
        "        ds\n",
        "        .cache()  # Cache elements in memory after the first pass\n",
        "        .shuffle(BUFFER_SIZE) # Shuffle the dataset. Use the full buffer size for better randomness\n",
        "        .batch(BATCH_SIZE) # Batch the elements\n",
        "        .prefetch(tf.data.AUTOTUNE) # Prefetch for optimized performance\n",
        "    )\n",
        "\n",
        "# 5. Apply the make_batches function to both the training and validation datasets\n",
        "train_batches = make_batches(train_dataset)\n",
        "val_batches = make_batches(val_dataset)\n",
        "\n",
        "print(\"make_batches function defined and applied to train_dataset and val_dataset.\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "make_batches function defined and applied to train_dataset and val_dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "388b7e3f"
      },
      "source": [
        "**Reasoning**:\n",
        "All previous steps have been successfully executed. The final instruction of the subtask is to verify the structure of one batch from the training dataset. This will confirm that the input pipelines are correctly configured.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eff7226a",
        "outputId": "ee1b66cf-31ad-4638-e46c-715a2370a2f6"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 6. Verify the structure of one batch from the training dataset\n",
        "# Iterate over one batch to inspect its elements and their shapes\n",
        "for eng_batch, spa_batch in train_batches.take(1):\n",
        "    print(\"\\n--- Verification of one training batch ---\")\n",
        "    print(f\"Shape of English batch: {eng_batch.shape}\")\n",
        "    print(f\"Shape of Spanish batch: {spa_batch.shape}\")\n",
        "    print(f\"Sample English batch (first element, first 5 tokens): {eng_batch[0][:5].numpy()}\")\n",
        "    print(f\"Sample Spanish batch (first element, first 5 tokens): {spa_batch[0][:5].numpy()}\")\n",
        "\n",
        "# Also check the validation batches\n",
        "for eng_batch_val, spa_batch_val in val_batches.take(1):\n",
        "    print(\"\\n--- Verification of one validation batch ---\")\n",
        "    print(f\"Shape of English validation batch: {eng_batch_val.shape}\")\n",
        "    print(f\"Shape of Spanish validation batch: {spa_batch_val.shape}\")\n",
        "    print(f\"Sample English validation batch (first element, first 5 tokens): {eng_batch_val[0][:5].numpy()}\")\n",
        "    print(f\"Sample Spanish validation batch (first element, first 5 tokens): {spa_batch_val[0][:5].numpy()}\")\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Verification of one training batch ---\n",
            "Shape of English batch: (64, 16)\n",
            "Shape of Spanish batch: (64, 17)\n",
            "Sample English batch (first element, first 5 tokens): [  2 462   7  23  26]\n",
            "Sample Spanish batch (first element, first 5 tokens): [   2  308   16 1098  681]\n",
            "\n",
            "--- Verification of one validation batch ---\n",
            "Shape of English validation batch: (64, 16)\n",
            "Shape of Spanish validation batch: (64, 17)\n",
            "Sample English validation batch (first element, first 5 tokens): [    2 11689  5467   302   999]\n",
            "Sample Spanish validation batch (first element, first 5 tokens): [   2   20  265 3334    1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a1378a0"
      },
      "source": [
        "**Reasoning**:\n",
        "All previous steps have been successfully executed. The final instruction of the subtask is to verify the structure of one batch from the training dataset. This will confirm that the input pipelines are correctly configured.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c145bc88",
        "outputId": "e05fc4e6-bacf-4ec1-9183-e4c998604ec0"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 6. Verify the structure of one batch from the training dataset\n",
        "# Iterate over one batch to inspect its elements and their shapes\n",
        "for eng_batch, spa_batch in train_batches.take(1):\n",
        "    print(\"\\n--- Verification of one training batch ---\")\n",
        "    print(f\"Shape of English batch: {eng_batch.shape}\")\n",
        "    print(f\"Shape of Spanish batch: {spa_batch.shape}\")\n",
        "    print(f\"Sample English batch (first element, first 5 tokens): {eng_batch[0][:5].numpy()}\")\n",
        "    print(f\"Sample Spanish batch (first element, first 5 tokens): {spa_batch[0][:5].numpy()}\")\n",
        "\n",
        "# Also check the validation batches\n",
        "for eng_batch_val, spa_batch_val in val_batches.take(1):\n",
        "    print(\"\\n--- Verification of one validation batch ---\")\n",
        "    print(f\"Shape of English validation batch: {eng_batch_val.shape}\")\n",
        "    print(f\"Shape of Spanish validation batch: {spa_batch_val.shape}\")\n",
        "    print(f\"Sample English validation batch (first element, first 5 tokens): {eng_batch_val[0][:5].numpy()}\")\n",
        "    print(f\"Sample Spanish validation batch (first element, first 5 tokens): {spa_batch_val[0][:5].numpy()}\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Verification of one training batch ---\n",
            "Shape of English batch: (64, 16)\n",
            "Shape of Spanish batch: (64, 17)\n",
            "Sample English batch (first element, first 5 tokens): [ 2 43 36 25 11]\n",
            "Sample Spanish batch (first element, first 5 tokens): [ 2  6 11 17  5]\n",
            "\n",
            "--- Verification of one validation batch ---\n",
            "Shape of English validation batch: (64, 16)\n",
            "Shape of Spanish validation batch: (64, 17)\n",
            "Sample English validation batch (first element, first 5 tokens): [ 2  4 22 34  6]\n",
            "Sample Spanish validation batch (first element, first 5 tokens): [   2    6 1165  162    4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4cf85d6"
      },
      "source": [
        "## Define Custom Loss Function and Metrics\n",
        "\n",
        "### Subtask:\n",
        "Implement the custom loss function (e.g., SparseCategoricalCrossentropy with masking) and evaluation metrics (e.g., SparseCategoricalAccuracy with masking) suitable for sequence-to-sequence tasks with padded sequences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c2b02f6"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to define the custom loss function as instructed. This function will wrap `tf.keras.losses.SparseCategoricalCrossentropy` and incorporate masking to ignore padded tokens (with ID 0) in the target sequence during loss calculation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da377dd0",
        "outputId": "6bae343a-b513-415a-8588-2ef4dac4f686"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Define a custom loss function\n",
        "def masked_loss(labels, logits):\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "    loss = loss_object(labels, logits)\n",
        "\n",
        "    # Create a mask for non-zero labels (non-padded tokens)\n",
        "    # The target token ID 0 is reserved for padding\n",
        "    mask = tf.cast(labels != 0, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "\n",
        "    # Calculate the sum of the unmasked loss values and the number of unmasked elements\n",
        "    # Then divide to get the average loss per unmasked element\n",
        "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "print(\"Custom masked_loss function defined.\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom masked_loss function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fe63d03"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to define a custom accuracy metric that ignores padded tokens (target token ID is 0) during calculation, adapting `tf.keras.metrics.SparseCategoricalAccuracy` for this purpose.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a9d1fe7",
        "outputId": "00ad9ebd-a93f-4972-9ee6-39c4438d81c0"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 2. Define a custom accuracy metric\n",
        "class MaskedSparseCategoricalAccuracy(tf.keras.metrics.SparseCategoricalAccuracy):\n",
        "    def __init__(self, name='masked_sparse_categorical_accuracy', dtype=None):\n",
        "        super().__init__(name, dtype=dtype)\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        # Create a mask for non-zero labels (non-padded tokens)\n",
        "        mask = tf.cast(y_true != 0, dtype=y_pred.dtype)\n",
        "        # If sample_weight is provided, combine it with our mask\n",
        "        if sample_weight is not None:\n",
        "            sample_weight = tf.cast(sample_weight, dtype=y_pred.dtype) * mask\n",
        "        else:\n",
        "            sample_weight = mask\n",
        "\n",
        "        super().update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "print(\"Custom MaskedSparseCategoricalAccuracy metric defined.\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom MaskedSparseCategoricalAccuracy metric defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d16abd2b"
      },
      "source": [
        "**Reasoning**:\n",
        "Both the custom loss function and the custom accuracy metric have been defined. The final step of the subtask is to test these functions with a sample batch to verify their correctness, especially regarding how they handle padded values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e98ed41"
      },
      "source": [
        "# Task\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Retrieve a sample batch from the validation dataset\n",
        "for eng_batch_val, spa_batch_val in val_batches.take(1):\n",
        "    y_true_batch = spa_batch_val\n",
        "    # We need to simulate logits from the final layer of the Transformer model.\n",
        "    # The shape should be (batch_size, sequence_length, vocab_size).\n",
        "    # The vocabulary size for Spanish is needed.\n",
        "    spanish_vocab = spanish_vectorizer.get_vocabulary()\n",
        "    spanish_vocab_size = len(spanish_vocab)\n",
        "\n",
        "    # Ensure MAX_SEQ_LENGTH_SPA is a Python integer (it already is from previous steps)\n",
        "    # Ensure BATCH_SIZE is a Python integer (it already is from previous steps)\n",
        "\n",
        "    # Create dummy logits for y_pred_batch\n",
        "    # Use the actual batch size from y_true_batch.shape[0] to handle potential partial batches\n",
        "    current_batch_size = y_true_batch.shape[0]\n",
        "    y_pred_batch = tf.random.uniform(\n",
        "        shape=(current_batch_size, MAX_SEQ_LENGTH_SPA, spanish_vocab_size),\n",
        "        minval=0.0,\n",
        "        maxval=1.0,\n",
        "        dtype=tf.float32\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Testing Custom Loss Function and Metric ---\")\n",
        "    print(f\"y_true_batch shape: {y_true_batch.shape}\")\n",
        "    print(f\"y_pred_batch (logits) shape: {y_pred_batch.shape}\")\n",
        "\n",
        "    # Test masked_loss\n",
        "    loss_value = masked_loss(y_true_batch, y_pred_batch)\n",
        "    print(f\"Calculated masked loss: {loss_value.numpy():.4f}\")\n",
        "\n",
        "    # Test MaskedSparseCategoricalAccuracy\n",
        "    metric = MaskedSparseCategoricalAccuracy()\n",
        "    metric.update_state(y_true_batch, y_pred_batch)\n",
        "    accuracy_value = metric.result()\n",
        "    print(f\"Calculated masked accuracy: {accuracy_value.numpy():.4f}\")\n",
        "\n",
        "    # Reset the metric state for future use if needed\n",
        "    metric.reset_state()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09f541a6"
      },
      "source": [
        "## Fix Tensor Type Mismatch in Loss/Accuracy Test\n",
        "\n",
        "### Subtask:\n",
        "Correct the `ValueError` in the test for `masked_loss` and `MaskedSparseCategoricalAccuracy` by converting TensorFlow scalar tensors to Python integers when constructing `tf.constant` indices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3088a775"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `masked_loss` function successfully computed a loss value for the dummy logits and true labels, indicating its functionality is operational.\n",
        "*   The `MaskedSparseCategoricalAccuracy` metric successfully updated its state and calculated an accuracy value, confirming its correct implementation.\n",
        "*   The successful execution of these tests, without the previously mentioned `ValueError` related to TensorFlow scalar tensors and `tf.constant` indices, confirms that the underlying type mismatch issue has been resolved.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The successful testing of `masked_loss` and `MaskedSparseCategoricalAccuracy` confirms the correction of the tensor type mismatch error, allowing these components to be reliably used in the model training pipeline.\n",
        "*   Proceed with integrating these validated loss and accuracy functions into the Transformer model's compilation and training process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06430e8a"
      },
      "source": [
        "# Task\n",
        "# Task\n",
        "Implement a custom learning rate schedule as described in the original Transformer paper, which typically involves a warmup phase followed by a decay. This schedule should be a subclass of `tf.keras.optimizers.schedules.LearningRateSchedule` and take `d_model` as an argument."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19589dc8"
      },
      "source": [
        "## Implement Custom Learning Rate Schedule\n",
        "\n",
        "### Subtask:\n",
        "Create a custom learning rate schedule as described in the original Transformer paper, which typically involves a warmup phase followed by a decay.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfb8bdfe"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires defining a custom learning rate schedule. The first step is to create the `CustomSchedule` class as specified in the instructions, implementing the `__init__`, `__call__`, and `get_config` methods with the given formulas and logic.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a14cc002",
        "outputId": "f9dfa202-91d4-4616-e19b-4356fb244859"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Define a class CustomSchedule that inherits from tf.keras.optimizers.schedules.LearningRateSchedule\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    # 2. In the __init__ method, initialize self.d_model and self.warmup_steps.\n",
        "    # Ensure d_model is cast to tf.cast(d_model, tf.float32).\n",
        "    self.d_model = tf.cast(d_model, tf.float32)\n",
        "    self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
        "\n",
        "  # 3. Implement the __call__ method, which takes step as an argument.\n",
        "  # Convert step to tf.cast(step, dtype=tf.float32).\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "\n",
        "    # 4. Calculate arg1 as tf.math.rsqrt(step).\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "\n",
        "    # 5. Calculate arg2 as step * (self.warmup_steps ** -1.5).\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    # 6. Return the learning rate using the formula: tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2).\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "  # 7. Implement the get_config method to return a dictionary containing d_model and warmup_steps for serialization.\n",
        "  def get_config(self):\n",
        "    return {\n",
        "        'd_model': self.d_model.numpy(),\n",
        "        'warmup_steps': self.warmup_steps.numpy()\n",
        "    }\n",
        "\n",
        "print(\"CustomSchedule class defined.\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomSchedule class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d03c9fec"
      },
      "source": [
        "## Configure and Compile Transformer Model\n",
        "\n",
        "### Subtask:\n",
        "Instantiate the `Transformer` model with appropriate hyperparameters (e.g., number of layers, `d_model`, number of heads, `dff`) based on the dataset characteristics and computational resources. Then, compile the model using the custom loss function, the custom learning rate, and the Adam optimizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93304e40"
      },
      "source": [
        "**Reasoning**:\n",
        "The first instruction is to define the hyperparameters for the Transformer model, including obtaining the vocabulary sizes from the adapted TextVectorization layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0414d180",
        "outputId": "6d851265-9574-48dc-b8ad-33f8de991a79"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Define hyperparameters for the Transformer model\n",
        "num_layers = 4\n",
        "d_model = 128  # Original paper used 512, but starting smaller for faster iteration\n",
        "num_heads = 8\n",
        "dff = 512 # Feed forward network dimension\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# Get vocabulary sizes from the adapted TextVectorization layers\n",
        "input_vocab_size = len(english_vectorizer.get_vocabulary())\n",
        "target_vocab_size = len(spanish_vectorizer.get_vocabulary())\n",
        "\n",
        "print(f\"num_layers: {num_layers}\")\n",
        "print(f\"d_model: {d_model}\")\n",
        "print(f\"num_heads: {num_heads}\")\n",
        "print(f\"dff: {dff}\")\n",
        "print(f\"dropout_rate: {dropout_rate}\")\n",
        "print(f\"input_vocab_size (English): {input_vocab_size}\")\n",
        "print(f\"target_vocab_size (Spanish): {target_vocab_size}\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_layers: 4\n",
            "d_model: 128\n",
            "num_heads: 8\n",
            "dff: 512\n",
            "dropout_rate: 0.1\n",
            "input_vocab_size (English): 14574\n",
            "target_vocab_size (Spanish): 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd772401"
      },
      "source": [
        "**Reasoning**:\n",
        "The next steps involve instantiating the Transformer model, creating the custom learning rate schedule, initializing the Adam optimizer with the custom schedule, and finally compiling the model with the defined loss function and metric.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37fbb6cf",
        "outputId": "70c21297-1d5c-4262-b258-bf2a80f9c8e6"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 2. Instantiate the Transformer model using the defined hyperparameters\n",
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=input_vocab_size,\n",
        "    target_vocab_size=target_vocab_size,\n",
        "    dropout_rate=dropout_rate)\n",
        "\n",
        "# 3. Create an instance of the CustomSchedule learning rate scheduler\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "# 4. Initialize the tf.keras.optimizers.Adam optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.98,\n",
        "    epsilon=1e-9)\n",
        "\n",
        "# 5. Compile the Transformer model\n",
        "transformer.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=masked_loss,\n",
        "    metrics=[MaskedSparseCategoricalAccuracy()])\n",
        "\n",
        "print(\"Transformer model instantiated, learning rate schedule and optimizer created, and model compiled.\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer model instantiated, learning rate schedule and optimizer created, and model compiled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18c48477"
      },
      "source": [
        "## Implement Training Loop\n",
        "\n",
        "### Subtask:\n",
        "Set up a custom training loop that iterates over the dataset for a specified number of epochs. Within each epoch, perform forward passes, calculate loss, compute gradients, and apply updates using the optimizer and the custom learning rate schedule. Log training progress, including loss and metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f1e4ba1"
      },
      "source": [
        "**Reasoning**:\n",
        "The first instruction is to define the number of training epochs, which is a necessary hyperparameter for the custom training loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ee5f5d8",
        "outputId": "dc3af649-90ff-41a8-8799-0820f7157c2a"
      },
      "source": [
        "EPOCHS = 5\n",
        "print(f\"Number of training epochs set to: {EPOCHS}\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training epochs set to: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f29a0990"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to create a custom training step function using `tf.function` as instructed, which will encapsulate the forward pass, loss calculation, gradient computation, and optimizer application for a single batch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc361236",
        "outputId": "4995adbf-d9f6-4261-e161-8c0ab0b6a780"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Initialize the metric outside the training step function\n",
        "train_accuracy_metric = MaskedSparseCategoricalAccuracy()\n",
        "\n",
        "@tf.function\n",
        "def train_step(eng_batch, spa_batch):\n",
        "    # Spanish input for the decoder (excluding the last token)\n",
        "    spa_input = spa_batch[:, :-1]\n",
        "    # Spanish target labels for loss calculation (excluding the first token)\n",
        "    spa_labels = spa_batch[:, 1:]\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Pass English batch as context (encoder input) and Spanish input (decoder input)\n",
        "        logits = transformer((eng_batch, spa_input))\n",
        "        loss = masked_loss(spa_labels, logits)\n",
        "\n",
        "    # Compute gradients\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    # Apply gradients\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    # Update the accuracy metric\n",
        "    train_accuracy_metric.update_state(spa_labels, logits)\n",
        "    return loss\n",
        "\n",
        "print(\"Custom training step function 'train_step' defined.\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom training step function 'train_step' defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3acf4cf1"
      },
      "source": [
        "**Reasoning**:\n",
        "The next instruction is to initialize lists to store training loss and accuracy history, which will be populated during the training loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ac0626d",
        "outputId": "40b5c5ca-ba93-428b-e49e-26e995d21b4a"
      },
      "source": [
        "train_loss_history = []\n",
        "train_accuracy_history = []\n",
        "\n",
        "print(\"Initialized lists for training loss and accuracy history.\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized lists for training loss and accuracy history.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "831398c4"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to implement the main training loop as instructed, iterating through the specified number of epochs, calling the `train_step` function for each batch, and logging progress.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33714bed",
        "outputId": "ed9749ea-7d69-41ca-a863-731ab95cf475"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "# Initialize metric for validation (outside the loop to prevent re-initialization per epoch)\n",
        "val_accuracy_metric = MaskedSparseCategoricalAccuracy()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "    train_accuracy_metric.reset_state() # Reset training accuracy at the start of each epoch\n",
        "\n",
        "    # Iterate through training batches\n",
        "    for (batch_num, (eng_batch, spa_batch)) in enumerate(train_batches):\n",
        "        loss = train_step(eng_batch, spa_batch)\n",
        "        total_train_loss += loss\n",
        "\n",
        "        # Print progress every few batches (e.g., every 50 batches)\n",
        "        if batch_num % 50 == 0:\n",
        "            current_lr = optimizer.learning_rate.numpy() # Access current learning rate directly\n",
        "            print(f'Epoch {epoch + 1} Batch {batch_num} LR: {current_lr:.6f} Loss: {loss.numpy():.4f} Accuracy: {train_accuracy_metric.result().numpy():.4f}')\n",
        "\n",
        "    # Calculate average training loss and accuracy for the epoch\n",
        "    avg_train_loss = total_train_loss / tf.data.experimental.cardinality(train_batches).numpy()\n",
        "    avg_train_accuracy = train_accuracy_metric.result().numpy()\n",
        "\n",
        "    train_loss_history.append(avg_train_loss)\n",
        "    train_accuracy_history.append(avg_train_accuracy)\n",
        "\n",
        "    # Evaluate on the validation set after each epoch\n",
        "    val_accuracy_metric.reset_state() # Reset validation accuracy\n",
        "    total_val_loss = 0\n",
        "    for (batch_num_val, (eng_batch_val, spa_batch_val)) in enumerate(val_batches):\n",
        "        # Spanish input for the decoder (excluding the last token)\n",
        "        spa_input_val = spa_batch_val[:, :-1]\n",
        "        # Spanish target labels for loss calculation (excluding the first token)\n",
        "        spa_labels_val = spa_batch_val[:, 1:]\n",
        "\n",
        "        logits_val = transformer((eng_batch_val, spa_input_val), training=False) # training=False for inference\n",
        "        val_loss = masked_loss(spa_labels_val, logits_val)\n",
        "        total_val_loss += val_loss\n",
        "        val_accuracy_metric.update_state(spa_labels_val, logits_val)\n",
        "\n",
        "    avg_val_loss = total_val_loss / tf.data.experimental.cardinality(val_batches).numpy()\n",
        "    avg_val_accuracy = val_accuracy_metric.result().numpy()\n",
        "\n",
        "    print(f'Epoch {epoch + 1} Training Loss: {avg_train_loss:.4f} Training Accuracy: {avg_train_accuracy:.4f} Validation Loss: {avg_val_loss:.4f} Validation Accuracy: {avg_val_accuracy:.4f} Time taken: {time.time() - start:.2f}s')\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 LR: 0.000000 Loss: 9.9136 Accuracy: 0.0000\n",
            "Epoch 1 Batch 50 LR: 0.000018 Loss: 9.7288 Accuracy: 0.0536\n",
            "Epoch 1 Batch 100 LR: 0.000035 Loss: 9.4758 Accuracy: 0.0974\n",
            "Epoch 1 Batch 150 LR: 0.000053 Loss: 9.1400 Accuracy: 0.1123\n",
            "Epoch 1 Batch 200 LR: 0.000070 Loss: 8.6821 Accuracy: 0.1203\n",
            "Epoch 1 Batch 250 LR: 0.000088 Loss: 8.1570 Accuracy: 0.1244\n",
            "Epoch 1 Batch 300 LR: 0.000105 Loss: 7.5358 Accuracy: 0.1276\n",
            "Epoch 1 Batch 350 LR: 0.000123 Loss: 7.0038 Accuracy: 0.1295\n",
            "Epoch 1 Batch 400 LR: 0.000140 Loss: 6.6907 Accuracy: 0.1313\n",
            "Epoch 1 Batch 450 LR: 0.000158 Loss: 6.2679 Accuracy: 0.1326\n",
            "Epoch 1 Batch 500 LR: 0.000175 Loss: 6.0421 Accuracy: 0.1336\n",
            "Epoch 1 Batch 550 LR: 0.000193 Loss: 5.8545 Accuracy: 0.1359\n",
            "Epoch 1 Batch 600 LR: 0.000210 Loss: 5.4672 Accuracy: 0.1407\n",
            "Epoch 1 Batch 650 LR: 0.000227 Loss: 5.5694 Accuracy: 0.1481\n",
            "Epoch 1 Batch 700 LR: 0.000245 Loss: 5.3310 Accuracy: 0.1574\n",
            "Epoch 1 Batch 750 LR: 0.000262 Loss: 5.0517 Accuracy: 0.1666\n",
            "Epoch 1 Batch 800 LR: 0.000280 Loss: 4.6021 Accuracy: 0.1753\n",
            "Epoch 1 Batch 850 LR: 0.000297 Loss: 4.7132 Accuracy: 0.1839\n",
            "Epoch 1 Batch 900 LR: 0.000315 Loss: 4.3895 Accuracy: 0.1925\n",
            "Epoch 1 Batch 950 LR: 0.000332 Loss: 4.3544 Accuracy: 0.2010\n",
            "Epoch 1 Batch 1000 LR: 0.000350 Loss: 4.2338 Accuracy: 0.2092\n",
            "Epoch 1 Batch 1050 LR: 0.000367 Loss: 4.3044 Accuracy: 0.2172\n",
            "Epoch 1 Batch 1100 LR: 0.000385 Loss: 3.9750 Accuracy: 0.2252\n",
            "Epoch 1 Batch 1150 LR: 0.000402 Loss: 4.1497 Accuracy: 0.2329\n",
            "Epoch 1 Batch 1200 LR: 0.000420 Loss: 3.9173 Accuracy: 0.2402\n",
            "Epoch 1 Batch 1250 LR: 0.000437 Loss: 3.5712 Accuracy: 0.2473\n",
            "Epoch 1 Batch 1300 LR: 0.000455 Loss: 3.8913 Accuracy: 0.2544\n",
            "Epoch 1 Batch 1350 LR: 0.000472 Loss: 3.6825 Accuracy: 0.2613\n",
            "Epoch 1 Batch 1400 LR: 0.000489 Loss: 3.3966 Accuracy: 0.2679\n",
            "Epoch 1 Batch 1450 LR: 0.000507 Loss: 3.1830 Accuracy: 0.2744\n",
            "Epoch 1 Batch 1500 LR: 0.000524 Loss: 3.4138 Accuracy: 0.2812\n",
            "Epoch 1 Batch 1550 LR: 0.000542 Loss: 3.1189 Accuracy: 0.2874\n",
            "Epoch 1 Batch 1600 LR: 0.000559 Loss: 3.2096 Accuracy: 0.2937\n",
            "Epoch 1 Batch 1650 LR: 0.000577 Loss: 3.3927 Accuracy: 0.2997\n",
            "Epoch 1 Batch 1700 LR: 0.000594 Loss: 3.4433 Accuracy: 0.3058\n",
            "Epoch 1 Batch 1750 LR: 0.000612 Loss: 2.8986 Accuracy: 0.3114\n",
            "Epoch 1 Training Loss: 5.2710 Training Accuracy: 0.3156 Validation Loss: 3.0691 Validation Accuracy: 0.5292 Time taken: 441.25s\n",
            "Epoch 2 Batch 0 LR: 0.000625 Loss: 2.5990 Accuracy: 0.5566\n",
            "Epoch 2 Batch 50 LR: 0.000642 Loss: 2.8973 Accuracy: 0.5379\n",
            "Epoch 2 Batch 100 LR: 0.000660 Loss: 2.9580 Accuracy: 0.5373\n",
            "Epoch 2 Batch 150 LR: 0.000677 Loss: 3.1039 Accuracy: 0.5414\n",
            "Epoch 2 Batch 200 LR: 0.000695 Loss: 2.6519 Accuracy: 0.5427\n",
            "Epoch 2 Batch 250 LR: 0.000712 Loss: 2.8837 Accuracy: 0.5445\n",
            "Epoch 2 Batch 300 LR: 0.000730 Loss: 3.1076 Accuracy: 0.5460\n",
            "Epoch 2 Batch 350 LR: 0.000747 Loss: 3.1847 Accuracy: 0.5481\n",
            "Epoch 2 Batch 400 LR: 0.000764 Loss: 2.6554 Accuracy: 0.5497\n",
            "Epoch 2 Batch 450 LR: 0.000782 Loss: 2.6117 Accuracy: 0.5517\n",
            "Epoch 2 Batch 500 LR: 0.000799 Loss: 2.7659 Accuracy: 0.5540\n",
            "Epoch 2 Batch 550 LR: 0.000817 Loss: 2.6776 Accuracy: 0.5561\n",
            "Epoch 2 Batch 600 LR: 0.000834 Loss: 2.5717 Accuracy: 0.5583\n",
            "Epoch 2 Batch 650 LR: 0.000852 Loss: 2.5962 Accuracy: 0.5606\n",
            "Epoch 2 Batch 700 LR: 0.000869 Loss: 2.7472 Accuracy: 0.5623\n",
            "Epoch 2 Batch 750 LR: 0.000887 Loss: 2.2972 Accuracy: 0.5642\n",
            "Epoch 2 Batch 800 LR: 0.000904 Loss: 2.4594 Accuracy: 0.5658\n",
            "Epoch 2 Batch 850 LR: 0.000922 Loss: 2.6254 Accuracy: 0.5673\n",
            "Epoch 2 Batch 900 LR: 0.000939 Loss: 2.3388 Accuracy: 0.5689\n",
            "Epoch 2 Batch 950 LR: 0.000957 Loss: 2.5480 Accuracy: 0.5703\n",
            "Epoch 2 Batch 1000 LR: 0.000974 Loss: 2.1995 Accuracy: 0.5719\n",
            "Epoch 2 Batch 1050 LR: 0.000992 Loss: 2.7823 Accuracy: 0.5734\n",
            "Epoch 2 Batch 1100 LR: 0.001009 Loss: 2.1239 Accuracy: 0.5744\n",
            "Epoch 2 Batch 1150 LR: 0.001026 Loss: 2.6111 Accuracy: 0.5758\n",
            "Epoch 2 Batch 1200 LR: 0.001044 Loss: 2.2977 Accuracy: 0.5767\n",
            "Epoch 2 Batch 1250 LR: 0.001061 Loss: 2.2277 Accuracy: 0.5779\n",
            "Epoch 2 Batch 1300 LR: 0.001079 Loss: 2.2749 Accuracy: 0.5789\n",
            "Epoch 2 Batch 1350 LR: 0.001096 Loss: 2.4778 Accuracy: 0.5800\n",
            "Epoch 2 Batch 1400 LR: 0.001114 Loss: 2.3202 Accuracy: 0.5809\n",
            "Epoch 2 Batch 1450 LR: 0.001131 Loss: 2.1748 Accuracy: 0.5820\n",
            "Epoch 2 Batch 1500 LR: 0.001149 Loss: 2.3256 Accuracy: 0.5830\n",
            "Epoch 2 Batch 1550 LR: 0.001166 Loss: 2.1152 Accuracy: 0.5839\n",
            "Epoch 2 Batch 1600 LR: 0.001184 Loss: 2.0516 Accuracy: 0.5850\n",
            "Epoch 2 Batch 1650 LR: 0.001201 Loss: 2.2287 Accuracy: 0.5858\n",
            "Epoch 2 Batch 1700 LR: 0.001219 Loss: 2.4291 Accuracy: 0.5866\n",
            "Epoch 2 Batch 1750 LR: 0.001236 Loss: 2.1840 Accuracy: 0.5877\n",
            "Epoch 2 Training Loss: 2.5344 Training Accuracy: 0.5883 Validation Loss: 2.3200 Validation Accuracy: 0.6183 Time taken: 401.48s\n",
            "Epoch 3 Batch 0 LR: 0.001249 Loss: 1.9021 Accuracy: 0.6681\n",
            "Epoch 3 Batch 50 LR: 0.001267 Loss: 2.0271 Accuracy: 0.6484\n",
            "Epoch 3 Batch 100 LR: 0.001284 Loss: 2.1262 Accuracy: 0.6425\n",
            "Epoch 3 Batch 150 LR: 0.001301 Loss: 1.9359 Accuracy: 0.6424\n",
            "Epoch 3 Batch 200 LR: 0.001319 Loss: 1.9246 Accuracy: 0.6400\n",
            "Epoch 3 Batch 250 LR: 0.001336 Loss: 1.8880 Accuracy: 0.6396\n",
            "Epoch 3 Batch 300 LR: 0.001354 Loss: 2.1837 Accuracy: 0.6399\n",
            "Epoch 3 Batch 350 LR: 0.001371 Loss: 2.0744 Accuracy: 0.6384\n",
            "Epoch 3 Batch 400 LR: 0.001389 Loss: 2.3100 Accuracy: 0.6384\n",
            "Epoch 3 Batch 450 LR: 0.001393 Loss: 2.2700 Accuracy: 0.6380\n",
            "Epoch 3 Batch 500 LR: 0.001385 Loss: 2.6726 Accuracy: 0.6377\n",
            "Epoch 3 Batch 550 LR: 0.001376 Loss: 2.2863 Accuracy: 0.6373\n",
            "Epoch 3 Batch 600 LR: 0.001368 Loss: 2.2361 Accuracy: 0.6373\n",
            "Epoch 3 Batch 650 LR: 0.001360 Loss: 2.1622 Accuracy: 0.6376\n",
            "Epoch 3 Batch 700 LR: 0.001352 Loss: 1.8296 Accuracy: 0.6381\n",
            "Epoch 3 Batch 750 LR: 0.001344 Loss: 1.8678 Accuracy: 0.6385\n",
            "Epoch 3 Batch 800 LR: 0.001336 Loss: 2.1214 Accuracy: 0.6383\n",
            "Epoch 3 Batch 850 LR: 0.001329 Loss: 1.7134 Accuracy: 0.6385\n",
            "Epoch 3 Batch 900 LR: 0.001321 Loss: 2.2111 Accuracy: 0.6391\n",
            "Epoch 3 Batch 950 LR: 0.001314 Loss: 1.9786 Accuracy: 0.6392\n",
            "Epoch 3 Batch 1000 LR: 0.001307 Loss: 2.1699 Accuracy: 0.6396\n",
            "Epoch 3 Batch 1050 LR: 0.001300 Loss: 2.3428 Accuracy: 0.6402\n",
            "Epoch 3 Batch 1100 LR: 0.001293 Loss: 2.0719 Accuracy: 0.6405\n",
            "Epoch 3 Batch 1150 LR: 0.001286 Loss: 1.6964 Accuracy: 0.6415\n",
            "Epoch 3 Batch 1200 LR: 0.001279 Loss: 1.8097 Accuracy: 0.6420\n",
            "Epoch 3 Batch 1250 LR: 0.001272 Loss: 1.9513 Accuracy: 0.6427\n",
            "Epoch 3 Batch 1300 LR: 0.001266 Loss: 1.9449 Accuracy: 0.6431\n",
            "Epoch 3 Batch 1350 LR: 0.001259 Loss: 1.7816 Accuracy: 0.6437\n",
            "Epoch 3 Batch 1400 LR: 0.001253 Loss: 1.8162 Accuracy: 0.6444\n",
            "Epoch 3 Batch 1450 LR: 0.001247 Loss: 1.7852 Accuracy: 0.6450\n",
            "Epoch 3 Batch 1500 LR: 0.001241 Loss: 2.1146 Accuracy: 0.6453\n",
            "Epoch 3 Batch 1550 LR: 0.001235 Loss: 1.6848 Accuracy: 0.6459\n",
            "Epoch 3 Batch 1600 LR: 0.001229 Loss: 2.0167 Accuracy: 0.6467\n",
            "Epoch 3 Batch 1650 LR: 0.001223 Loss: 1.8388 Accuracy: 0.6472\n",
            "Epoch 3 Batch 1700 LR: 0.001217 Loss: 1.8403 Accuracy: 0.6479\n",
            "Epoch 3 Batch 1750 LR: 0.001211 Loss: 1.7665 Accuracy: 0.6485\n",
            "Epoch 3 Training Loss: 2.0306 Training Accuracy: 0.6489 Validation Loss: 1.9910 Validation Accuracy: 0.6611 Time taken: 401.73s\n",
            "Epoch 4 Batch 0 LR: 0.001207 Loss: 1.7588 Accuracy: 0.6724\n",
            "Epoch 4 Batch 50 LR: 0.001201 Loss: 1.7285 Accuracy: 0.6918\n",
            "Epoch 4 Batch 100 LR: 0.001196 Loss: 1.7665 Accuracy: 0.6919\n",
            "Epoch 4 Batch 150 LR: 0.001191 Loss: 1.9580 Accuracy: 0.6943\n",
            "Epoch 4 Batch 200 LR: 0.001185 Loss: 1.8048 Accuracy: 0.6954\n",
            "Epoch 4 Batch 250 LR: 0.001180 Loss: 1.7833 Accuracy: 0.6957\n",
            "Epoch 4 Batch 300 LR: 0.001175 Loss: 1.5556 Accuracy: 0.6962\n",
            "Epoch 4 Batch 350 LR: 0.001170 Loss: 1.8462 Accuracy: 0.6958\n",
            "Epoch 4 Batch 400 LR: 0.001164 Loss: 1.7636 Accuracy: 0.6952\n",
            "Epoch 4 Batch 450 LR: 0.001159 Loss: 1.6369 Accuracy: 0.6946\n",
            "Epoch 4 Batch 500 LR: 0.001154 Loss: 1.7618 Accuracy: 0.6945\n",
            "Epoch 4 Batch 550 LR: 0.001150 Loss: 1.7501 Accuracy: 0.6948\n",
            "Epoch 4 Batch 600 LR: 0.001145 Loss: 1.7123 Accuracy: 0.6951\n",
            "Epoch 4 Batch 650 LR: 0.001140 Loss: 1.7858 Accuracy: 0.6945\n",
            "Epoch 4 Batch 700 LR: 0.001135 Loss: 1.6017 Accuracy: 0.6944\n",
            "Epoch 4 Batch 750 LR: 0.001131 Loss: 1.5566 Accuracy: 0.6948\n",
            "Epoch 4 Batch 800 LR: 0.001126 Loss: 1.5316 Accuracy: 0.6949\n",
            "Epoch 4 Batch 850 LR: 0.001121 Loss: 1.7143 Accuracy: 0.6954\n",
            "Epoch 4 Batch 900 LR: 0.001117 Loss: 1.8111 Accuracy: 0.6955\n",
            "Epoch 4 Batch 950 LR: 0.001113 Loss: 1.9088 Accuracy: 0.6956\n",
            "Epoch 4 Batch 1000 LR: 0.001108 Loss: 1.5968 Accuracy: 0.6957\n",
            "Epoch 4 Batch 1050 LR: 0.001104 Loss: 1.8786 Accuracy: 0.6955\n",
            "Epoch 4 Batch 1100 LR: 0.001100 Loss: 1.6200 Accuracy: 0.6956\n",
            "Epoch 4 Batch 1150 LR: 0.001095 Loss: 1.7989 Accuracy: 0.6956\n",
            "Epoch 4 Batch 1200 LR: 0.001091 Loss: 1.8733 Accuracy: 0.6958\n",
            "Epoch 4 Batch 1250 LR: 0.001087 Loss: 1.7420 Accuracy: 0.6960\n",
            "Epoch 4 Batch 1300 LR: 0.001083 Loss: 1.4128 Accuracy: 0.6959\n",
            "Epoch 4 Batch 1350 LR: 0.001079 Loss: 1.4942 Accuracy: 0.6962\n",
            "Epoch 4 Batch 1400 LR: 0.001075 Loss: 1.9236 Accuracy: 0.6965\n",
            "Epoch 4 Batch 1450 LR: 0.001071 Loss: 1.4187 Accuracy: 0.6968\n",
            "Epoch 4 Batch 1500 LR: 0.001067 Loss: 1.7783 Accuracy: 0.6969\n",
            "Epoch 4 Batch 1550 LR: 0.001063 Loss: 1.7239 Accuracy: 0.6970\n",
            "Epoch 4 Batch 1600 LR: 0.001059 Loss: 1.5903 Accuracy: 0.6973\n",
            "Epoch 4 Batch 1650 LR: 0.001056 Loss: 1.8226 Accuracy: 0.6976\n",
            "Epoch 4 Batch 1700 LR: 0.001052 Loss: 1.6609 Accuracy: 0.6978\n",
            "Epoch 4 Batch 1750 LR: 0.001048 Loss: 1.6371 Accuracy: 0.6981\n",
            "Epoch 4 Training Loss: 1.7063 Training Accuracy: 0.6982 Validation Loss: 1.8346 Validation Accuracy: 0.6892 Time taken: 402.65s\n",
            "Epoch 5 Batch 0 LR: 0.001045 Loss: 1.3446 Accuracy: 0.7605\n",
            "Epoch 5 Batch 50 LR: 0.001042 Loss: 1.2532 Accuracy: 0.7360\n",
            "Epoch 5 Batch 100 LR: 0.001038 Loss: 1.3652 Accuracy: 0.7356\n",
            "Epoch 5 Batch 150 LR: 0.001035 Loss: 1.3902 Accuracy: 0.7345\n",
            "Epoch 5 Batch 200 LR: 0.001031 Loss: 1.5152 Accuracy: 0.7339\n",
            "Epoch 5 Batch 250 LR: 0.001028 Loss: 1.5594 Accuracy: 0.7324\n",
            "Epoch 5 Batch 300 LR: 0.001024 Loss: 1.3987 Accuracy: 0.7315\n",
            "Epoch 5 Batch 350 LR: 0.001021 Loss: 1.4724 Accuracy: 0.7311\n",
            "Epoch 5 Batch 400 LR: 0.001017 Loss: 1.4106 Accuracy: 0.7313\n",
            "Epoch 5 Batch 450 LR: 0.001014 Loss: 1.5237 Accuracy: 0.7302\n",
            "Epoch 5 Batch 500 LR: 0.001011 Loss: 1.4653 Accuracy: 0.7295\n",
            "Epoch 5 Batch 550 LR: 0.001007 Loss: 1.4987 Accuracy: 0.7293\n",
            "Epoch 5 Batch 600 LR: 0.001004 Loss: 1.4198 Accuracy: 0.7292\n",
            "Epoch 5 Batch 650 LR: 0.001001 Loss: 1.3601 Accuracy: 0.7293\n",
            "Epoch 5 Batch 700 LR: 0.000998 Loss: 1.5993 Accuracy: 0.7295\n",
            "Epoch 5 Batch 750 LR: 0.000995 Loss: 1.6997 Accuracy: 0.7294\n",
            "Epoch 5 Batch 800 LR: 0.000991 Loss: 1.6052 Accuracy: 0.7289\n",
            "Epoch 5 Batch 850 LR: 0.000988 Loss: 1.7017 Accuracy: 0.7289\n",
            "Epoch 5 Batch 900 LR: 0.000985 Loss: 1.6053 Accuracy: 0.7288\n",
            "Epoch 5 Batch 950 LR: 0.000982 Loss: 1.5192 Accuracy: 0.7287\n",
            "Epoch 5 Batch 1000 LR: 0.000979 Loss: 1.3823 Accuracy: 0.7288\n",
            "Epoch 5 Batch 1050 LR: 0.000976 Loss: 1.3889 Accuracy: 0.7290\n",
            "Epoch 5 Batch 1100 LR: 0.000973 Loss: 1.1009 Accuracy: 0.7286\n",
            "Epoch 5 Batch 1150 LR: 0.000970 Loss: 1.3382 Accuracy: 0.7284\n",
            "Epoch 5 Batch 1200 LR: 0.000967 Loss: 1.5714 Accuracy: 0.7283\n",
            "Epoch 5 Batch 1250 LR: 0.000964 Loss: 1.6550 Accuracy: 0.7283\n",
            "Epoch 5 Batch 1300 LR: 0.000962 Loss: 1.5213 Accuracy: 0.7282\n",
            "Epoch 5 Batch 1350 LR: 0.000959 Loss: 1.6100 Accuracy: 0.7281\n",
            "Epoch 5 Batch 1400 LR: 0.000956 Loss: 1.4572 Accuracy: 0.7279\n",
            "Epoch 5 Batch 1450 LR: 0.000953 Loss: 1.5535 Accuracy: 0.7280\n",
            "Epoch 5 Batch 1500 LR: 0.000950 Loss: 1.5206 Accuracy: 0.7279\n",
            "Epoch 5 Batch 1550 LR: 0.000948 Loss: 1.6799 Accuracy: 0.7279\n",
            "Epoch 5 Batch 1600 LR: 0.000945 Loss: 1.6715 Accuracy: 0.7279\n",
            "Epoch 5 Batch 1650 LR: 0.000942 Loss: 1.6418 Accuracy: 0.7280\n",
            "Epoch 5 Batch 1700 LR: 0.000940 Loss: 1.6849 Accuracy: 0.7281\n",
            "Epoch 5 Batch 1750 LR: 0.000937 Loss: 1.7891 Accuracy: 0.7281\n",
            "Epoch 5 Training Loss: 1.5296 Training Accuracy: 0.7280 Validation Loss: 1.7717 Validation Accuracy: 0.7024 Time taken: 400.70s\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "63ba0f87",
        "outputId": "176fc9e1-3cca-4090-c104-6b7e3b27accd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot training loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot([t.numpy() for t in train_loss_history], label='Training Loss')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot training accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracy_history, label='Training Accuracy')\n",
        "plt.title('Training Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "814022d6",
        "outputId": "c5e67d2b-3a05-49ab-e903-9dc0e7215c92"
      },
      "source": [
        "# Initialize a new metric for evaluation on the test/validation set\n",
        "test_accuracy_metric = MaskedSparseCategoricalAccuracy()\n",
        "total_test_loss = 0\n",
        "\n",
        "print(\"Evaluating model on the validation set...\")\n",
        "\n",
        "for (batch_num_test, (eng_batch_test, spa_batch_test)) in enumerate(val_batches):\n",
        "    # Spanish input for the decoder (excluding the last token)\n",
        "    spa_input_test = spa_batch_test[:, :-1]\n",
        "    # Spanish target labels for loss calculation (excluding the first token)\n",
        "    spa_labels_test = spa_batch_test[:, 1:]\n",
        "\n",
        "    # Perform a forward pass in inference mode\n",
        "    logits_test = transformer((eng_batch_test, spa_input_test), training=False)\n",
        "\n",
        "    # Calculate loss and update accuracy\n",
        "    loss_test = masked_loss(spa_labels_test, logits_test)\n",
        "    total_test_loss += loss_test\n",
        "    test_accuracy_metric.update_state(spa_labels_test, logits_test)\n",
        "\n",
        "# Calculate average test loss and accuracy\n",
        "avg_test_loss = total_test_loss / tf.data.experimental.cardinality(val_batches).numpy()\n",
        "avg_test_accuracy = test_accuracy_metric.result().numpy()\n",
        "\n",
        "print(f\"\\nFinal Test Loss: {avg_test_loss:.4f}\")\n",
        "print(f\"Final Test Accuracy: {avg_test_accuracy:.4f}\")\n",
        "\n",
        "# Reset the metric state for future use if needed\n",
        "test_accuracy_metric.reset_state()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model on the validation set...\n",
            "\n",
            "Final Test Loss: 1.7707\n",
            "Final Test Accuracy: 0.7024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "259082b8"
      },
      "source": [
        "# Task\n",
        "```python\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# 4. Implement the main training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "    train_accuracy_metric.reset_state()  # Reset metric at the beginning of each epoch\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
        "    for (batch_num, (eng_batch, spa_batch)) in enumerate(train_batches):\n",
        "        loss = train_step(eng_batch, spa_batch)\n",
        "        total_loss += loss\n",
        "\n",
        "        if batch_num % 50 == 0:\n",
        "            current_learning_rate = optimizer.learning_rate.numpy() # Access current learning rate directly\n",
        "            print(f\"Batch {batch_num:4d} Loss: {loss.numpy():.4f} \"\n",
        "                  f\"Accuracy: {train_accuracy_metric.result().numpy():.4f} \"\n",
        "                  f\"LR: {current_learning_rate:.6f}\")\n",
        "\n",
        "    # Calculate average loss and accuracy for the epoch\n",
        "    avg_epoch_loss = total_loss / (batch_num + 1)\n",
        "    epoch_accuracy = train_accuracy_metric.result().numpy()\n",
        "\n",
        "    train_loss_history.append(avg_epoch_loss.numpy())\n",
        "    train_accuracy_history.append(epoch_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} finished in {time.time() - start:.2f} seconds\")\n",
        "    print(f\"Epoch {epoch + 1} Avg Loss: {avg_epoch_loss:.4f} \"\n",
        "          f\"Avg Accuracy: {epoch_accuracy:.4f}\")\n",
        "\n",
        "    # Optional: Add validation step here if val_batches is available\n",
        "    # For now, we are focusing on the training loop itself.\n",
        "\n",
        "print(\"\\nTraining complete.\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e07bd8e0"
      },
      "source": [
        "## Fix Learning Rate Access in Training Loop\n",
        "\n",
        "### Subtask:\n",
        "Modify the training loop to correctly access the current learning rate from the optimizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f871a936"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The training loop was modified to correctly access and display the current learning rate during training.\n",
        "*   The learning rate is now directly retrieved using `optimizer.learning_rate.numpy()` within each batch iteration, ensuring that the printed learning rate accurately reflects its current value, particularly useful for schedules that dynamically adjust the learning rate.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   This fix ensures accurate monitoring of the learning rate, which is crucial for debugging and understanding the optimization process, especially when using learning rate schedules.\n",
        "*   The next step could involve verifying that the displayed learning rate changes as expected when a learning rate schedule is applied, or integrating validation steps to monitor model performance with respect to the training metrics and learning rate.\n"
      ]
    }
  ]
}